{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6fd23c3e",
   "metadata": {},
   "source": [
    "## Applying Artificial Neural Network Algorithms for forecasting (Stock Time Series Forecasting)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0bb25b",
   "metadata": {},
   "source": [
    "# HYBRID MODEL X3 v01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "815c8982",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - loss: 811948416.0000 - val_loss: 747123584.0000 - learning_rate: 0.0050\n",
      "Epoch 2/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 803004608.0000 - val_loss: 746193280.0000 - learning_rate: 0.0050\n",
      "Epoch 3/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 797700160.0000 - val_loss: 745292096.0000 - learning_rate: 0.0050\n",
      "Epoch 4/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 735405568.0000 - val_loss: 744400576.0000 - learning_rate: 0.0050\n",
      "Epoch 5/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 764508992.0000 - val_loss: 743511488.0000 - learning_rate: 0.0050\n",
      "Epoch 6/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 791566976.0000 - val_loss: 742627072.0000 - learning_rate: 0.0050\n",
      "Epoch 7/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 737245312.0000 - val_loss: 741736000.0000 - learning_rate: 0.0050\n",
      "Epoch 8/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 753567616.0000 - val_loss: 740853120.0000 - learning_rate: 0.0050\n",
      "Epoch 9/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 744287936.0000 - val_loss: 739972224.0000 - learning_rate: 0.0050\n",
      "Epoch 10/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 767245056.0000 - val_loss: 739093888.0000 - learning_rate: 0.0050\n",
      "Epoch 11/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 806084800.0000 - val_loss: 738228544.0000 - learning_rate: 0.0050\n",
      "Epoch 12/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 787691392.0000 - val_loss: 737363776.0000 - learning_rate: 0.0050\n",
      "Epoch 13/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 778720768.0000 - val_loss: 736504384.0000 - learning_rate: 0.0050\n",
      "Epoch 14/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 743766080.0000 - val_loss: 735644672.0000 - learning_rate: 0.0050\n",
      "Epoch 15/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 785325184.0000 - val_loss: 734791040.0000 - learning_rate: 0.0050\n",
      "Epoch 16/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 769906048.0000 - val_loss: 733923200.0000 - learning_rate: 0.0050\n",
      "Epoch 17/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 802996928.0000 - val_loss: 733062720.0000 - learning_rate: 0.0050\n",
      "Epoch 18/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 761186176.0000 - val_loss: 732210304.0000 - learning_rate: 0.0050\n",
      "Epoch 19/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 791559936.0000 - val_loss: 731329664.0000 - learning_rate: 0.0050\n",
      "Epoch 20/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 783241984.0000 - val_loss: 730459328.0000 - learning_rate: 0.0050\n",
      "Epoch 21/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 794490432.0000 - val_loss: 729610624.0000 - learning_rate: 0.0050\n",
      "Epoch 22/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 766589056.0000 - val_loss: 728777152.0000 - learning_rate: 0.0050\n",
      "Epoch 23/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 758236160.0000 - val_loss: 727927488.0000 - learning_rate: 0.0050\n",
      "Epoch 24/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 750657152.0000 - val_loss: 727080384.0000 - learning_rate: 0.0050\n",
      "Epoch 25/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 748036800.0000 - val_loss: 726272192.0000 - learning_rate: 0.0050\n",
      "Epoch 26/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 791248768.0000 - val_loss: 725365248.0000 - learning_rate: 0.0050\n",
      "Epoch 27/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 807007424.0000 - val_loss: 724520832.0000 - learning_rate: 0.0050\n",
      "Epoch 28/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 739182976.0000 - val_loss: 723676928.0000 - learning_rate: 0.0050\n",
      "Epoch 29/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 762730176.0000 - val_loss: 722824704.0000 - learning_rate: 0.0050\n",
      "Epoch 30/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 773015168.0000 - val_loss: 721990272.0000 - learning_rate: 0.0050\n",
      "Epoch 31/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 760580672.0000 - val_loss: 721150528.0000 - learning_rate: 0.0050\n",
      "Epoch 32/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 741832192.0000 - val_loss: 720313792.0000 - learning_rate: 0.0050\n",
      "Epoch 33/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 747611456.0000 - val_loss: 719491072.0000 - learning_rate: 0.0050\n",
      "Epoch 34/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 777799424.0000 - val_loss: 718656512.0000 - learning_rate: 0.0050\n",
      "Epoch 35/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 764566976.0000 - val_loss: 717826560.0000 - learning_rate: 0.0050\n",
      "Epoch 36/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 738019776.0000 - val_loss: 717011776.0000 - learning_rate: 0.0050\n",
      "Epoch 37/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 763240192.0000 - val_loss: 716171840.0000 - learning_rate: 0.0050\n",
      "Epoch 38/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 739384640.0000 - val_loss: 715346880.0000 - learning_rate: 0.0050\n",
      "Epoch 39/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 737444672.0000 - val_loss: 714528896.0000 - learning_rate: 0.0050\n",
      "Epoch 40/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 757259200.0000 - val_loss: 713687424.0000 - learning_rate: 0.0050\n",
      "Epoch 41/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 731826496.0000 - val_loss: 712862720.0000 - learning_rate: 0.0050\n",
      "Epoch 42/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 739424384.0000 - val_loss: 712052608.0000 - learning_rate: 0.0050\n",
      "Epoch 43/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 705855808.0000 - val_loss: 711236032.0000 - learning_rate: 0.0050\n",
      "Epoch 44/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 740757760.0000 - val_loss: 710422080.0000 - learning_rate: 0.0050\n",
      "Epoch 45/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 739800960.0000 - val_loss: 709609664.0000 - learning_rate: 0.0050\n",
      "Epoch 46/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 753832832.0000 - val_loss: 708804224.0000 - learning_rate: 0.0050\n",
      "Epoch 47/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 709115904.0000 - val_loss: 707989952.0000 - learning_rate: 0.0050\n",
      "Epoch 48/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 735443648.0000 - val_loss: 707157696.0000 - learning_rate: 0.0050\n",
      "Epoch 49/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 740066432.0000 - val_loss: 706362688.0000 - learning_rate: 0.0050\n",
      "Epoch 50/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 694732160.0000 - val_loss: 705570880.0000 - learning_rate: 0.0050\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step\n",
      "Mean Squared Error: 705570888.7253455\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, BatchNormalization, SimpleRNN, LSTM, Dense, Dropout, Reshape\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from ta.momentum import RSIIndicator\n",
    "from ta.volatility import BollingerBands\n",
    "from ta.trend import MACD\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Define ticker symbol and data range\n",
    "ticker = \"BTC-USD\"\n",
    "start = \"2000-01-01\"\n",
    "end = yf.Ticker(ticker).history(period=\"5d\").index[0].strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Download historical data\n",
    "df = yf.download(ticker, start=start, end=end)\n",
    "\n",
    "# INDICATORS\n",
    "rsi_indicator = RSIIndicator(df['Close'], window=14)\n",
    "bb_indicator = BollingerBands(df['Close'], window=20)\n",
    "\n",
    "# Feature engineering / number of features\n",
    "def create_features(df):\n",
    "    df['Change'] = df['Close'] - df['Open']\n",
    "    df['Pct_Change'] = df['Change'] / df['Open'] * 100\n",
    "    df['Moving_Avg_5'] = df['Close'].rolling(window=5).mean()\n",
    "    df['Moving_Avg_10'] = df['Close'].rolling(window=10).mean()\n",
    "    df['Moving_Avg_50'] = df['Close'].rolling(window=50).mean()\n",
    "    df['Moving_Avg_200'] = df['Close'].rolling(window=200).mean()\n",
    "    df['Exp_Moving_Avg'] = df['Close'].ewm(span=10, adjust=False).mean()\n",
    "    df['Bollinger_Bands'] = bb_indicator.bollinger_mavg()\n",
    "    df['RSI'] = rsi_indicator.rsi()\n",
    "    return df\n",
    "\n",
    "df = create_features(df.copy())\n",
    "\n",
    "# Remove rows with NaN values\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Split data into features (X) and target (y)\n",
    "X = df.drop(['Close'], axis=1)\n",
    "y = df['Close']\n",
    "\n",
    "# Impute missing values using mean imputation\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_imputed = imputer.fit_transform(X)\n",
    "\n",
    "# Normalize the data\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "X_scaled = scaler.fit_transform(X_imputed)\n",
    "\n",
    "# Check the number of features in X_scaled\n",
    "n_samples = X_scaled.shape[0]\n",
    "n_features = X_scaled.shape[1]\n",
    "\n",
    "# Choose the number of timesteps, for example, 7 timesteps\n",
    "n_timesteps = 7  \n",
    "n_features_per_timestep = X_scaled.shape[1] // n_timesteps\n",
    "\n",
    "# Ensure the number of features is divisible by the number of timesteps\n",
    "if n_features % n_timesteps != 0:\n",
    "    raise ValueError(f\"Number of features ({n_features}) is not divisible by timesteps ({n_timesteps})\")\n",
    "\n",
    "# Reshape X_scaled to (samples, timesteps, features_per_timestep)\n",
    "X_scaled = np.reshape(X_scaled, (X_scaled.shape[0], n_timesteps, n_features_per_timestep))\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Build CNN + RNN + LSTM hybrid model with batch normalization and no Flatten\n",
    "model = Sequential()\n",
    "\n",
    "# 1. CNN layer: Extracts features from time series\n",
    "model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(BatchNormalization()) \n",
    "\n",
    "# 2. RNN layer: Captures short-term dependencies\n",
    "model.add(SimpleRNN(units=32, return_sequences=True)) # 50\n",
    "model.add(BatchNormalization())  \n",
    "\n",
    "# 3. LSTM layers: Learns long-term dependencies\n",
    "model.add(LSTM(units=256, return_sequences=True)) \n",
    "model.add(Dropout(0.3))  \n",
    "model.add(LSTM(units=128)) #64\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "# 4. Output layer: Predict the closing price\n",
    "model.add(Dense(units=1))\n",
    "\n",
    "# Compile the model with Adam optimizer and a lower learning rate\n",
    "# optimizer / Adam, RMSprop, Adagrad\n",
    "optimizer = Adam(learning_rate=0.005) # 0.0005, 0.0001\n",
    "model.compile(optimizer=optimizer, loss='mse')\n",
    "\n",
    "# Add a learning rate scheduler to reduce the learning rate dynamically\n",
    "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-5, verbose=1)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, np.reshape(y_train.values, (-1, 1)), epochs=50, batch_size=64, validation_data=(X_test, np.reshape(y_test.values, (-1, 1))), callbacks=[lr_scheduler])\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Evaluate model performance\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "print(f\"Mean Squared Error: {mse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c08718",
   "metadata": {},
   "source": [
    "# HYBRID MODEL X3 v02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1d84b03f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "C:\\Users\\2014\\AppData\\Local\\Temp\\ipykernel_17636\\3370329163.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['Price_Change_Pct'] = df['Close'].pct_change()\n",
      "C:\\Users\\2014\\AppData\\Local\\Temp\\ipykernel_17636\\3370329163.py:35: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['Log_Returns'] = np.log(df['Close'] / df['Close'].shift(1))\n",
      "C:\\Users\\2014\\AppData\\Local\\Temp\\ipykernel_17636\\3370329163.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['50d_MA'] = df['Close'].rolling(window=50).mean()\n",
      "C:\\Users\\2014\\AppData\\Local\\Temp\\ipykernel_17636\\3370329163.py:37: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['200d_MA'] = df['Close'].rolling(window=200).mean()\n",
      "C:\\Users\\2014\\AppData\\Local\\Temp\\ipykernel_17636\\3370329163.py:41: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['RSI'] = rsi_indicator.rsi()\n",
      "C:\\Users\\2014\\AppData\\Local\\Temp\\ipykernel_17636\\3370329163.py:43: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['Bollinger_Bands'] = bb_indicator.bollinger_mavg()\n",
      "C:\\Users\\2014\\AppData\\Local\\Temp\\ipykernel_17636\\3370329163.py:45: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['MACD'] = macd_indicator.macd()\n",
      "C:\\Users\\2014\\AppData\\Local\\Temp\\ipykernel_17636\\3370329163.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['MACD_Signal'] = macd_indicator.macd_signal()\n",
      "c:\\Users\\2014\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 156ms/step - loss: 717744192.0000 - val_loss: 872070848.0000\n",
      "Epoch 2/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 740058048.0000 - val_loss: 871774144.0000\n",
      "Epoch 3/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 642087040.0000 - val_loss: 871615616.0000\n",
      "Epoch 4/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 717784832.0000 - val_loss: 871453760.0000\n",
      "Epoch 5/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 694204096.0000 - val_loss: 871290880.0000\n",
      "Epoch 6/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 727273664.0000 - val_loss: 871128896.0000\n",
      "Epoch 7/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 727602176.0000 - val_loss: 870968704.0000\n",
      "Epoch 8/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 669311104.0000 - val_loss: 870809536.0000\n",
      "Epoch 9/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 713343808.0000 - val_loss: 870647744.0000\n",
      "Epoch 10/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 667501568.0000 - val_loss: 870488000.0000\n",
      "Epoch 11/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 640621632.0000 - val_loss: 870329600.0000\n",
      "Epoch 12/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 706067072.0000 - val_loss: 870173312.0000\n",
      "Epoch 13/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 691934592.0000 - val_loss: 870016896.0000\n",
      "Epoch 14/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 751516864.0000 - val_loss: 869860992.0000\n",
      "Epoch 15/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 730011136.0000 - val_loss: 869707648.0000\n",
      "Epoch 16/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 698696704.0000 - val_loss: 869556160.0000\n",
      "Epoch 17/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 743928512.0000 - val_loss: 869403648.0000\n",
      "Epoch 18/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 669038336.0000 - val_loss: 869252096.0000\n",
      "Epoch 19/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 693778624.0000 - val_loss: 869099776.0000\n",
      "Epoch 20/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 737713024.0000 - val_loss: 868948672.0000\n",
      "Epoch 21/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 704779776.0000 - val_loss: 868800192.0000\n",
      "Epoch 22/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 680177408.0000 - val_loss: 868647808.0000\n",
      "Epoch 23/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 698375424.0000 - val_loss: 868495168.0000\n",
      "Epoch 24/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 672999680.0000 - val_loss: 868343040.0000\n",
      "Epoch 25/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 631500352.0000 - val_loss: 868189760.0000\n",
      "Epoch 26/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 664652480.0000 - val_loss: 868035264.0000\n",
      "Epoch 27/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 686552896.0000 - val_loss: 867882816.0000\n",
      "Epoch 28/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 695200832.0000 - val_loss: 867728832.0000\n",
      "Epoch 29/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 751155456.0000 - val_loss: 867575168.0000\n",
      "Epoch 30/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 687080512.0000 - val_loss: 867425600.0000\n",
      "Epoch 31/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 652485056.0000 - val_loss: 867276096.0000\n",
      "Epoch 32/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 725238656.0000 - val_loss: 867125248.0000\n",
      "Epoch 33/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 710954752.0000 - val_loss: 866975680.0000\n",
      "Epoch 34/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 766310784.0000 - val_loss: 866825920.0000\n",
      "Epoch 35/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 689299712.0000 - val_loss: 866678528.0000\n",
      "Epoch 36/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 644300288.0000 - val_loss: 866530048.0000\n",
      "Epoch 37/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 662524928.0000 - val_loss: 866381376.0000\n",
      "Epoch 38/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 776677376.0000 - val_loss: 866229632.0000\n",
      "Epoch 39/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 706118592.0000 - val_loss: 866080832.0000\n",
      "Epoch 40/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 769046464.0000 - val_loss: 865928640.0000\n",
      "Epoch 41/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 670414656.0000 - val_loss: 865781824.0000\n",
      "Epoch 42/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 720778176.0000 - val_loss: 865634944.0000\n",
      "Epoch 43/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 698670016.0000 - val_loss: 865488704.0000\n",
      "Epoch 44/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 620401664.0000 - val_loss: 865345344.0000\n",
      "Epoch 45/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 693747328.0000 - val_loss: 865198208.0000\n",
      "Epoch 46/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 739845184.0000 - val_loss: 865050304.0000\n",
      "Epoch 47/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 683750336.0000 - val_loss: 864904512.0000\n",
      "Epoch 48/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 665115840.0000 - val_loss: 864757824.0000\n",
      "Epoch 49/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 680438592.0000 - val_loss: 864609728.0000\n",
      "Epoch 50/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 719329984.0000 - val_loss: 864459840.0000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 137ms/step\n",
      "Mean Squared Error: 876175529.4433936\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, SimpleRNN, LSTM, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from ta.momentum import RSIIndicator\n",
    "from ta.volatility import BollingerBands\n",
    "from ta.trend import MACD\n",
    "from scipy.stats import zscore\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Define ticker symbol and data range\n",
    "ticker = \"BTC-USD\"\n",
    "start = \"2000-01-01\"\n",
    "end = (datetime.today() - timedelta(days=5)).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Download historical data\n",
    "df = yf.download(ticker, start=start, end=end)\n",
    "\n",
    "# Preprocess the data\n",
    "df['Date'] = pd.to_datetime(df.index)\n",
    "df['Date'] = df['Date'].apply(lambda date: date.timestamp())\n",
    "\n",
    "# Remove noise and outliers\n",
    "z_scores = zscore(df[['Open', 'High', 'Low', 'Close', 'Volume']])\n",
    "df = df[(z_scores < 3).all(axis=1)]\n",
    "\n",
    "# Feature engineering\n",
    "df['Price_Change_Pct'] = df['Close'].pct_change()\n",
    "df['Log_Returns'] = np.log(df['Close'] / df['Close'].shift(1))\n",
    "df['50d_MA'] = df['Close'].rolling(window=50).mean()\n",
    "df['200d_MA'] = df['Close'].rolling(window=200).mean()\n",
    "\n",
    "# Technical indicators\n",
    "rsi_indicator = RSIIndicator(df['Close'], window=14)\n",
    "df['RSI'] = rsi_indicator.rsi()\n",
    "bb_indicator = BollingerBands(df['Close'], window=20)\n",
    "df['Bollinger_Bands'] = bb_indicator.bollinger_mavg()\n",
    "macd_indicator = MACD(df['Close'], window_slow=26, window_fast=12)\n",
    "df['MACD'] = macd_indicator.macd()\n",
    "df['MACD_Signal'] = macd_indicator.macd_signal()\n",
    "\n",
    "# Remove NA\n",
    "df = df.dropna()\n",
    "\n",
    "# Split the data into features and target\n",
    "X = df[['Price_Change_Pct', 'Log_Returns', '50d_MA', '200d_MA', 'RSI', 'Bollinger_Bands', 'MACD']]\n",
    "y = df['Close']\n",
    "\n",
    "# Impute missing values using mean imputation\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_imputed = imputer.fit_transform(X)\n",
    "\n",
    "# Normalize the data\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "X_scaled = scaler.fit_transform(X_imputed)\n",
    "\n",
    "n_features = X_scaled.shape[1]\n",
    "\n",
    "# Reshape X_scaled for CNN-RNN input\n",
    "n_timesteps = 7  # Set timesteps based on sequence length\n",
    "n_samples = (X_scaled.shape[0] // n_timesteps) * n_timesteps \n",
    "\n",
    "# Truncate the data so it can be reshaped\n",
    "X_scaled = X_scaled[:n_samples, :]  # Truncate to be divisible by n_timesteps\n",
    "y = y.iloc[:n_samples] \n",
    "\n",
    "# Reshape the data\n",
    "X_scaled = X_scaled.reshape((n_samples // n_timesteps, n_timesteps, n_features))\n",
    "\n",
    "# Ensure target shape matches the X input sequence\n",
    "y = y.values.reshape((-1, n_timesteps, 1))  # Match the shape of target for each timestep\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Build the hybrid model\n",
    "model = Sequential()\n",
    "\n",
    "# 1. CNN layer: Extracts features from time series\n",
    "model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(BatchNormalization())  \n",
    "\n",
    "# 2. RNN layer: Captures short-term dependencies\n",
    "model.add(SimpleRNN(units=32, return_sequences=True))\n",
    "model.add(BatchNormalization()) \n",
    "\n",
    "# 3. LSTM layers: Learns long-term dependencies\n",
    "model.add(LSTM(units=256, return_sequences=True))  \n",
    "model.add(Dropout(0.3))  \n",
    "model.add(LSTM(units=128))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "# 4. Output layer: Predict the closing price\n",
    "model.add(Dense(units=1))\n",
    "\n",
    "# Compile the model with Adam optimizer and a lower learning rate\n",
    "optimizer = Adam(learning_rate=0.005)\n",
    "model.compile(optimizer=optimizer, loss='mse')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=64, validation_data=(X_test, y_test))\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Since the model predicts one value per sequence, select only the last timestep from y_test\n",
    "y_test_last = y_test[:, -1]  \n",
    "\n",
    "# Evaluate performance using the last timestep values\n",
    "mse = mean_squared_error(y_test_last.flatten(), predictions.flatten())\n",
    "print(f\"Mean Squared Error: {mse}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
