{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f0bb25b",
   "metadata": {},
   "source": [
    "# MODEL NN 01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "815c8982",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 52ms/step - loss: 793677568.0000 - val_loss: 715307008.0000 - learning_rate: 0.0050\n",
      "Epoch 2/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 805496192.0000 - val_loss: 714395648.0000 - learning_rate: 0.0050\n",
      "Epoch 3/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 764160896.0000 - val_loss: 713501376.0000 - learning_rate: 0.0050\n",
      "Epoch 4/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 819097664.0000 - val_loss: 712612416.0000 - learning_rate: 0.0050\n",
      "Epoch 5/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 800596864.0000 - val_loss: 711757824.0000 - learning_rate: 0.0050\n",
      "Epoch 6/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 796674304.0000 - val_loss: 710896128.0000 - learning_rate: 0.0050\n",
      "Epoch 7/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 772096000.0000 - val_loss: 710057088.0000 - learning_rate: 0.0050\n",
      "Epoch 8/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 768410624.0000 - val_loss: 709208832.0000 - learning_rate: 0.0050\n",
      "Epoch 9/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 783141568.0000 - val_loss: 708350592.0000 - learning_rate: 0.0050\n",
      "Epoch 10/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 768473216.0000 - val_loss: 707496448.0000 - learning_rate: 0.0050\n",
      "Epoch 11/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 784251904.0000 - val_loss: 706654784.0000 - learning_rate: 0.0050\n",
      "Epoch 12/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 770577856.0000 - val_loss: 705825152.0000 - learning_rate: 0.0050\n",
      "Epoch 13/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 754084608.0000 - val_loss: 704985024.0000 - learning_rate: 0.0050\n",
      "Epoch 14/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 785513728.0000 - val_loss: 704136960.0000 - learning_rate: 0.0050\n",
      "Epoch 15/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 765972032.0000 - val_loss: 703284160.0000 - learning_rate: 0.0050\n",
      "Epoch 16/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 751767424.0000 - val_loss: 702432064.0000 - learning_rate: 0.0050\n",
      "Epoch 17/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 761695680.0000 - val_loss: 701601728.0000 - learning_rate: 0.0050\n",
      "Epoch 18/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 769751872.0000 - val_loss: 700767232.0000 - learning_rate: 0.0050\n",
      "Epoch 19/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 813074176.0000 - val_loss: 699939648.0000 - learning_rate: 0.0050\n",
      "Epoch 20/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 733134784.0000 - val_loss: 699106368.0000 - learning_rate: 0.0050\n",
      "Epoch 21/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 762086976.0000 - val_loss: 698280576.0000 - learning_rate: 0.0050\n",
      "Epoch 22/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 798737984.0000 - val_loss: 697440576.0000 - learning_rate: 0.0050\n",
      "Epoch 23/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 738028480.0000 - val_loss: 696611328.0000 - learning_rate: 0.0050\n",
      "Epoch 24/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 724333696.0000 - val_loss: 695793152.0000 - learning_rate: 0.0050\n",
      "Epoch 25/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 748151808.0000 - val_loss: 694987840.0000 - learning_rate: 0.0050\n",
      "Epoch 26/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 762764416.0000 - val_loss: 694171392.0000 - learning_rate: 0.0050\n",
      "Epoch 27/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 762356800.0000 - val_loss: 693352640.0000 - learning_rate: 0.0050\n",
      "Epoch 28/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 770342528.0000 - val_loss: 692546368.0000 - learning_rate: 0.0050\n",
      "Epoch 29/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 814977472.0000 - val_loss: 691710976.0000 - learning_rate: 0.0050\n",
      "Epoch 30/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 748689088.0000 - val_loss: 690913280.0000 - learning_rate: 0.0050\n",
      "Epoch 31/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 765149312.0000 - val_loss: 690114624.0000 - learning_rate: 0.0050\n",
      "Epoch 32/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 723039616.0000 - val_loss: 689302016.0000 - learning_rate: 0.0050\n",
      "Epoch 33/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 798287744.0000 - val_loss: 688481216.0000 - learning_rate: 0.0050\n",
      "Epoch 34/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 759732736.0000 - val_loss: 687682304.0000 - learning_rate: 0.0050\n",
      "Epoch 35/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 776073088.0000 - val_loss: 686865088.0000 - learning_rate: 0.0050\n",
      "Epoch 36/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 772941248.0000 - val_loss: 686065536.0000 - learning_rate: 0.0050\n",
      "Epoch 37/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 775221184.0000 - val_loss: 685257600.0000 - learning_rate: 0.0050\n",
      "Epoch 38/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 747019904.0000 - val_loss: 684464128.0000 - learning_rate: 0.0050\n",
      "Epoch 39/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 774827648.0000 - val_loss: 683661504.0000 - learning_rate: 0.0050\n",
      "Epoch 40/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 773316736.0000 - val_loss: 682861824.0000 - learning_rate: 0.0050\n",
      "Epoch 41/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 773300864.0000 - val_loss: 682035456.0000 - learning_rate: 0.0050\n",
      "Epoch 42/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 751274112.0000 - val_loss: 681257792.0000 - learning_rate: 0.0050\n",
      "Epoch 43/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 774698048.0000 - val_loss: 680463488.0000 - learning_rate: 0.0050\n",
      "Epoch 44/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 780823424.0000 - val_loss: 679682944.0000 - learning_rate: 0.0050\n",
      "Epoch 45/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 736074432.0000 - val_loss: 678895616.0000 - learning_rate: 0.0050\n",
      "Epoch 46/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 756795200.0000 - val_loss: 678098240.0000 - learning_rate: 0.0050\n",
      "Epoch 47/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 761481600.0000 - val_loss: 677300352.0000 - learning_rate: 0.0050\n",
      "Epoch 48/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 731325632.0000 - val_loss: 676501632.0000 - learning_rate: 0.0050\n",
      "Epoch 49/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 762427520.0000 - val_loss: 675711424.0000 - learning_rate: 0.0050\n",
      "Epoch 50/50\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 748856128.0000 - val_loss: 674926464.0000 - learning_rate: 0.0050\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 28ms/step\n",
      "Mean Squared Error: 674926427.6104312\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, BatchNormalization, SimpleRNN, LSTM, Dense, Dropout, Reshape\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from ta.momentum import RSIIndicator\n",
    "from ta.volatility import BollingerBands\n",
    "from ta.trend import MACD\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Define ticker symbol and data range\n",
    "ticker = \"BTC-USD\"\n",
    "start = \"2000-01-01\"\n",
    "end = yf.Ticker(ticker).history(period=\"5d\").index[0].strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Download historical data\n",
    "df = yf.download(ticker, start=start, end=end)\n",
    "\n",
    "# INDICATORS\n",
    "rsi_indicator = RSIIndicator(df['Close'], window=14)\n",
    "bb_indicator = BollingerBands(df['Close'], window=20)\n",
    "\n",
    "# Feature engineering / number of features\n",
    "def create_features(df):\n",
    "    df['Change'] = df['Close'] - df['Open']\n",
    "    df['Pct_Change'] = df['Change'] / df['Open'] * 100\n",
    "    df['Moving_Avg_5'] = df['Close'].rolling(window=5).mean()\n",
    "    df['Moving_Avg_10'] = df['Close'].rolling(window=10).mean()\n",
    "    df['Moving_Avg_50'] = df['Close'].rolling(window=50).mean()\n",
    "    df['Moving_Avg_200'] = df['Close'].rolling(window=200).mean()\n",
    "    df['Exp_Moving_Avg'] = df['Close'].ewm(span=10, adjust=False).mean()\n",
    "    df['Bollinger_Bands'] = bb_indicator.bollinger_mavg()\n",
    "    df['RSI'] = rsi_indicator.rsi()\n",
    "    return df\n",
    "\n",
    "df = create_features(df.copy())\n",
    "\n",
    "# Remove rows with NaN values\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Split data into features (X) and target (y)\n",
    "X = df.drop(['Close'], axis=1)\n",
    "y = df['Close']\n",
    "\n",
    "# Impute missing values using mean imputation\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_imputed = imputer.fit_transform(X)\n",
    "\n",
    "# Normalize the data\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "X_scaled = scaler.fit_transform(X_imputed)\n",
    "\n",
    "# Check the number of features in X_scaled\n",
    "n_samples = X_scaled.shape[0]\n",
    "n_features = X_scaled.shape[1]\n",
    "\n",
    "# Choose the number of timesteps, for example, 7 timesteps\n",
    "n_timesteps = 7  # Increase timesteps to 7 for better temporal structure\n",
    "n_features_per_timestep = X_scaled.shape[1] // n_timesteps\n",
    "\n",
    "# Ensure the number of features is divisible by the number of timesteps\n",
    "if n_features % n_timesteps != 0:\n",
    "    raise ValueError(f\"Number of features ({n_features}) is not divisible by timesteps ({n_timesteps})\")\n",
    "\n",
    "# Reshape X_scaled to (samples, timesteps, features_per_timestep)\n",
    "X_scaled = np.reshape(X_scaled, (X_scaled.shape[0], n_timesteps, n_features_per_timestep))\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Build CNN + RNN + LSTM hybrid model with batch normalization and no Flatten\n",
    "model = Sequential()\n",
    "\n",
    "# 1. CNN layer: Extracts features from time series\n",
    "model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(BatchNormalization())  # Added batch normalization after CNN\n",
    "\n",
    "# 2. RNN layer: Captures short-term dependencies\n",
    "model.add(SimpleRNN(units=32, return_sequences=True)) # 50\n",
    "model.add(BatchNormalization())  # Batch normalization to stabilize RNN layer\n",
    "\n",
    "# 3. LSTM layers: Learns long-term dependencies\n",
    "model.add(LSTM(units=256, return_sequences=True))  # Increased LSTM units for better memory 128\n",
    "model.add(Dropout(0.3))  # Increased dropout rate to avoid overfitting\n",
    "model.add(LSTM(units=128)) #64\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "# 4. Output layer: Predict the closing price\n",
    "model.add(Dense(units=1))\n",
    "\n",
    "# Compile the model with Adam optimizer and a lower learning rate\n",
    "# optimizer / Adam, RMSprop, Adagrad\n",
    "optimizer = Adam(learning_rate=0.005) # 0.0005, 0.0001\n",
    "model.compile(optimizer=optimizer, loss='mse')\n",
    "\n",
    "# Add a learning rate scheduler to reduce the learning rate dynamically\n",
    "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-5, verbose=1)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, np.reshape(y_train.values, (-1, 1)), epochs=50, batch_size=64, validation_data=(X_test, np.reshape(y_test.values, (-1, 1))), callbacks=[lr_scheduler])\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Evaluate model performance\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "print(f\"Mean Squared Error: {mse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c08718",
   "metadata": {},
   "source": [
    "# MODEL NN 02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1d84b03f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 202ms/step - loss: 661631872.0000 - val_loss: 864700800.0000\n",
      "Epoch 2/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 727073792.0000 - val_loss: 864389312.0000\n",
      "Epoch 3/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 666940608.0000 - val_loss: 864233280.0000\n",
      "Epoch 4/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 733420416.0000 - val_loss: 864073280.0000\n",
      "Epoch 5/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 663612096.0000 - val_loss: 863913216.0000\n",
      "Epoch 6/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 735389440.0000 - val_loss: 863750528.0000\n",
      "Epoch 7/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 718261312.0000 - val_loss: 863590720.0000\n",
      "Epoch 8/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 665427712.0000 - val_loss: 863432320.0000\n",
      "Epoch 9/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 721227648.0000 - val_loss: 863270464.0000\n",
      "Epoch 10/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 678505600.0000 - val_loss: 863111552.0000\n",
      "Epoch 11/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 738794816.0000 - val_loss: 862954880.0000\n",
      "Epoch 12/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 691221056.0000 - val_loss: 862798976.0000\n",
      "Epoch 13/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 732177728.0000 - val_loss: 862641920.0000\n",
      "Epoch 14/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 657704128.0000 - val_loss: 862489600.0000\n",
      "Epoch 15/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 754125376.0000 - val_loss: 862333952.0000\n",
      "Epoch 16/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 762179200.0000 - val_loss: 862181632.0000\n",
      "Epoch 17/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 679788224.0000 - val_loss: 862032640.0000\n",
      "Epoch 18/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 678810176.0000 - val_loss: 861882304.0000\n",
      "Epoch 19/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 702500416.0000 - val_loss: 861731072.0000\n",
      "Epoch 20/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 737935744.0000 - val_loss: 861579584.0000\n",
      "Epoch 21/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 688443264.0000 - val_loss: 861428608.0000\n",
      "Epoch 22/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 734627008.0000 - val_loss: 861276416.0000\n",
      "Epoch 23/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 688094400.0000 - val_loss: 861125632.0000\n",
      "Epoch 24/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 688608512.0000 - val_loss: 860976704.0000\n",
      "Epoch 25/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 679210560.0000 - val_loss: 860826624.0000\n",
      "Epoch 26/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 690847232.0000 - val_loss: 860672832.0000\n",
      "Epoch 27/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 749765888.0000 - val_loss: 860521088.0000\n",
      "Epoch 28/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 654675648.0000 - val_loss: 860370944.0000\n",
      "Epoch 29/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 690708608.0000 - val_loss: 860218944.0000\n",
      "Epoch 30/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 657554496.0000 - val_loss: 860068288.0000\n",
      "Epoch 31/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 726849728.0000 - val_loss: 859914880.0000\n",
      "Epoch 32/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 689597952.0000 - val_loss: 859765056.0000\n",
      "Epoch 33/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 649737216.0000 - val_loss: 859616128.0000\n",
      "Epoch 34/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 784760320.0000 - val_loss: 859463872.0000\n",
      "Epoch 35/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 674309952.0000 - val_loss: 859317760.0000\n",
      "Epoch 36/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 721229824.0000 - val_loss: 859171008.0000\n",
      "Epoch 37/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 678686080.0000 - val_loss: 859020288.0000\n",
      "Epoch 38/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 685904960.0000 - val_loss: 858871168.0000\n",
      "Epoch 39/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 755797376.0000 - val_loss: 858721216.0000\n",
      "Epoch 40/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 707689728.0000 - val_loss: 858574464.0000\n",
      "Epoch 41/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 725633024.0000 - val_loss: 858425728.0000\n",
      "Epoch 42/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 652031424.0000 - val_loss: 858279616.0000\n",
      "Epoch 43/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 701191360.0000 - val_loss: 858130688.0000\n",
      "Epoch 44/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 701847552.0000 - val_loss: 857981888.0000\n",
      "Epoch 45/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 667297664.0000 - val_loss: 857833728.0000\n",
      "Epoch 46/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 752033728.0000 - val_loss: 857686720.0000\n",
      "Epoch 47/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 695409024.0000 - val_loss: 857540864.0000\n",
      "Epoch 48/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 725652544.0000 - val_loss: 857395008.0000\n",
      "Epoch 49/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 645606848.0000 - val_loss: 857250752.0000\n",
      "Epoch 50/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 675340416.0000 - val_loss: 857099520.0000\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 193ms/step\n",
      "Mean Squared Error: 834520259.4728857\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, SimpleRNN, LSTM, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from ta.momentum import RSIIndicator\n",
    "from ta.volatility import BollingerBands\n",
    "from ta.trend import MACD\n",
    "from scipy.stats import zscore\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Define ticker symbol and data range\n",
    "ticker = \"BTC-USD\"\n",
    "start = \"2000-01-01\"\n",
    "end = (datetime.today() - timedelta(days=5)).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Download historical data\n",
    "df = yf.download(ticker, start=start, end=end)\n",
    "\n",
    "# Preprocess the data\n",
    "df['Date'] = pd.to_datetime(df.index)\n",
    "df['Date'] = df['Date'].apply(lambda date: date.timestamp())\n",
    "\n",
    "# Remove noise and outliers\n",
    "z_scores = zscore(df[['Open', 'High', 'Low', 'Close', 'Volume']])\n",
    "df = df[(z_scores < 3).all(axis=1)]\n",
    "\n",
    "# Feature engineering\n",
    "df['Price_Change_Pct'] = df['Close'].pct_change()\n",
    "df['Log_Returns'] = np.log(df['Close'] / df['Close'].shift(1))\n",
    "df['50d_MA'] = df['Close'].rolling(window=50).mean()\n",
    "df['200d_MA'] = df['Close'].rolling(window=200).mean()\n",
    "\n",
    "# Technical indicators\n",
    "rsi_indicator = RSIIndicator(df['Close'], window=14)\n",
    "df['RSI'] = rsi_indicator.rsi()\n",
    "bb_indicator = BollingerBands(df['Close'], window=20)\n",
    "df['Bollinger_Bands'] = bb_indicator.bollinger_mavg()\n",
    "macd_indicator = MACD(df['Close'], window_slow=26, window_fast=12)\n",
    "df['MACD'] = macd_indicator.macd()\n",
    "df['MACD_Signal'] = macd_indicator.macd_signal()\n",
    "\n",
    "# Remove NA\n",
    "df = df.dropna()\n",
    "\n",
    "# Split the data into features and target\n",
    "X = df[['Price_Change_Pct', 'Log_Returns', '50d_MA', '200d_MA', 'RSI', 'Bollinger_Bands', 'MACD']]\n",
    "y = df['Close']\n",
    "\n",
    "# Impute missing values using mean imputation\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_imputed = imputer.fit_transform(X)\n",
    "\n",
    "# Normalize the data\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "X_scaled = scaler.fit_transform(X_imputed)\n",
    "\n",
    "n_features = X_scaled.shape[1]\n",
    "\n",
    "# Reshape X_scaled for CNN-RNN input\n",
    "n_timesteps = 7  # Set timesteps based on sequence length\n",
    "n_samples = (X_scaled.shape[0] // n_timesteps) * n_timesteps  # Ensure divisible by n_timesteps\n",
    "\n",
    "# Truncate the data so it can be reshaped\n",
    "X_scaled = X_scaled[:n_samples, :]  # Truncate to be divisible by n_timesteps\n",
    "y = y.iloc[:n_samples]  # Truncate y to match X\n",
    "\n",
    "# Reshape the data\n",
    "X_scaled = X_scaled.reshape((n_samples // n_timesteps, n_timesteps, n_features))\n",
    "\n",
    "# Ensure target shape matches the X input sequence\n",
    "y = y.values.reshape((-1, n_timesteps, 1))  # Match the shape of target for each timestep\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Build the hybrid model\n",
    "model = Sequential()\n",
    "\n",
    "# 1. CNN layer: Extracts features from time series\n",
    "model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(BatchNormalization())  # Added batch normalization after CNN\n",
    "\n",
    "# 2. RNN layer: Captures short-term dependencies\n",
    "model.add(SimpleRNN(units=32, return_sequences=True))\n",
    "model.add(BatchNormalization())  # Batch normalization to stabilize RNN layer\n",
    "\n",
    "# 3. LSTM layers: Learns long-term dependencies\n",
    "model.add(LSTM(units=256, return_sequences=True))  # Increased LSTM units for better memory\n",
    "model.add(Dropout(0.3))  # Increased dropout rate to avoid overfitting\n",
    "model.add(LSTM(units=128))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "# 4. Output layer: Predict the closing price\n",
    "model.add(Dense(units=1))\n",
    "\n",
    "# Compile the model with Adam optimizer and a lower learning rate\n",
    "optimizer = Adam(learning_rate=0.005)\n",
    "model.compile(optimizer=optimizer, loss='mse')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=64, validation_data=(X_test, y_test))\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Since the model predicts one value per sequence, select only the last timestep from y_test\n",
    "y_test_last = y_test[:, -1]  # Last value of each sequence\n",
    "\n",
    "# Evaluate performance using the last timestep values\n",
    "mse = mean_squared_error(y_test_last.flatten(), predictions.flatten())\n",
    "print(f\"Mean Squared Error: {mse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08c9a97",
   "metadata": {},
   "source": [
    "# MODEL NN 03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c2933f48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 329ms/step - loss: 93930.2969 - val_loss: 164444.1719\n",
      "Epoch 2/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 85989.1797 - val_loss: 159156.6562\n",
      "Epoch 3/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 83020.3516 - val_loss: 156149.9375\n",
      "Epoch 4/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 80757.5234 - val_loss: 153465.6562\n",
      "Epoch 5/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 77485.8281 - val_loss: 150800.9688\n",
      "Epoch 6/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 75516.4766 - val_loss: 148161.4531\n",
      "Epoch 7/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 78992.1094 - val_loss: 145556.2188\n",
      "Epoch 8/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 73794.3672 - val_loss: 143012.8281\n",
      "Epoch 9/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 73517.9609 - val_loss: 140510.1250\n",
      "Epoch 10/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 69662.2188 - val_loss: 138061.7812\n",
      "Epoch 11/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 65424.5430 - val_loss: 135667.3594\n",
      "Epoch 12/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 65323.3711 - val_loss: 133307.0625\n",
      "Epoch 13/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 63992.3086 - val_loss: 130990.2891\n",
      "Epoch 14/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 63070.8750 - val_loss: 128711.9375\n",
      "Epoch 15/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 59018.0625 - val_loss: 126486.7891\n",
      "Epoch 16/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 59808.7305 - val_loss: 124292.3672\n",
      "Epoch 17/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 57892.3750 - val_loss: 122138.7500\n",
      "Epoch 18/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 56872.6953 - val_loss: 120025.6328\n",
      "Epoch 19/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 55766.7383 - val_loss: 117946.3906\n",
      "Epoch 20/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 54871.0234 - val_loss: 115905.2422\n",
      "Epoch 21/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 54946.1719 - val_loss: 113896.4922\n",
      "Epoch 22/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 51465.0391 - val_loss: 111929.8984\n",
      "Epoch 23/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 53157.0156 - val_loss: 109990.6406\n",
      "Epoch 24/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 47423.1797 - val_loss: 108097.0312\n",
      "Epoch 25/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 45672.1797 - val_loss: 106231.5234\n",
      "Epoch 26/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 47103.9922 - val_loss: 104384.0312\n",
      "Epoch 27/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 45651.6680 - val_loss: 102575.4688\n",
      "Epoch 28/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 43281.0117 - val_loss: 100801.5547\n",
      "Epoch 29/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 43621.7266 - val_loss: 99055.7812\n",
      "Epoch 30/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 40020.9492 - val_loss: 97347.0312\n",
      "Epoch 31/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 42265.7383 - val_loss: 95652.5938\n",
      "Epoch 32/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 40442.4023 - val_loss: 93995.5000\n",
      "Epoch 33/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 37660.3516 - val_loss: 92372.7266\n",
      "Epoch 34/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 39118.1875 - val_loss: 90768.8594\n",
      "Epoch 35/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 37212.1328 - val_loss: 89197.9688\n",
      "Epoch 36/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 35097.3516 - val_loss: 87656.5391\n",
      "Epoch 37/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 36158.7500 - val_loss: 86137.5781\n",
      "Epoch 38/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 36404.2969 - val_loss: 84644.0625\n",
      "Epoch 39/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 33395.9414 - val_loss: 83183.4609\n",
      "Epoch 40/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 34122.0391 - val_loss: 81744.6328\n",
      "Epoch 41/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 31402.0195 - val_loss: 80335.6562\n",
      "Epoch 42/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 30927.3789 - val_loss: 78948.9922\n",
      "Epoch 43/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 31163.7793 - val_loss: 77583.6328\n",
      "Epoch 44/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 29912.1758 - val_loss: 76245.6016\n",
      "Epoch 45/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 28976.3477 - val_loss: 74931.2578\n",
      "Epoch 46/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 28089.3652 - val_loss: 73639.7578\n",
      "Epoch 47/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 27927.7246 - val_loss: 72370.1016\n",
      "Epoch 48/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 26232.0723 - val_loss: 71130.5781\n",
      "Epoch 49/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 25076.8301 - val_loss: 69915.4453\n",
      "Epoch 50/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 25667.9160 - val_loss: 68718.3906\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 946ms/step\n",
      "Mean Squared Error: 68718.38341973654\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, SimpleRNN, LSTM, Dense, Dropout, Reshape, BatchNormalization, Bidirectional\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from ta.momentum import RSIIndicator\n",
    "from ta.volatility import BollingerBands\n",
    "from ta.trend import MACD\n",
    "from scipy.stats import zscore\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "# Define ticker symbol and data range\n",
    "ticker = \"BTC-USD\"\n",
    "start = \"2000-01-01\"\n",
    "end = (datetime.today() - timedelta(days=5)).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Download historical data\n",
    "df = yf.download(ticker, start=start, end=end)\n",
    "\n",
    "# Preprocess the data\n",
    "df['Date'] = pd.to_datetime(df.index)\n",
    "df['Date'] = df['Date'].apply(lambda date: date.timestamp())\n",
    "\n",
    "# Remove noise and outliers\n",
    "z_scores = zscore(df[['Open', 'High', 'Low', 'Close', 'Volume']])\n",
    "df = df[(z_scores < 3).all(axis=1)]\n",
    "\n",
    "# Feature engineering\n",
    "df['Price_Change_Pct'] = df['Close'].pct_change()\n",
    "df['Log_Returns'] = np.log(df['Close'] / df['Close'].shift(1))\n",
    "df['50d_MA'] = df['Close'].rolling(window=50).mean()\n",
    "df['200d_MA'] = df['Close'].rolling(window=200).mean()\n",
    "\n",
    "# Technical indicators\n",
    "rsi_indicator = RSIIndicator(df['Close'], window=14)\n",
    "df['RSI'] = rsi_indicator.rsi()\n",
    "bb_indicator = BollingerBands(df['Close'], window=20)\n",
    "df['Bollinger_Bands'] = bb_indicator.bollinger_mavg()\n",
    "macd_indicator = MACD(df['Close'], window_slow=26, window_fast=12)\n",
    "df['MACD'] = macd_indicator.macd()\n",
    "df['MACD_Signal'] = macd_indicator.macd_signal()\n",
    "\n",
    "# Remove NA\n",
    "df = df.dropna()\n",
    "\n",
    "# Split the data into features and target\n",
    "X = df[['Price_Change_Pct', 'Log_Returns', '50d_MA', '200d_MA', 'RSI', 'Bollinger_Bands', 'MACD']]\n",
    "y = df['Close']\n",
    "\n",
    "# Impute missing values using mean imputation\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_imputed = imputer.fit_transform(X)\n",
    "\n",
    "# Normalize the data\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "X_scaled = scaler.fit_transform(X_imputed)\n",
    "\n",
    "# Reshape X_scaled to fit CNN input\n",
    "n_timesteps = 10\n",
    "n_features = X_scaled.shape[1]\n",
    "X_scaled = np.reshape(X_scaled, (-1, n_timesteps, n_features))\n",
    "\n",
    "# Split data into training and testing sets\n",
    "tscv = TimeSeriesSplit(n_splits=10)\n",
    "for train_index, test_index in tscv.split(X_scaled):\n",
    "    X_train, X_test = X_scaled[train_index], X_scaled[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "# Build the hybrid model\n",
    "model = Sequential()\n",
    "\n",
    "# 1. CNN layer: Extracts features from time series\n",
    "model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# 2. Bidirectional RNN layer: Captures past and future dependencies\n",
    "model.add(Bidirectional(SimpleRNN(units=64, return_sequences=True)))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# 3. LSTM layers: Learns long-term dependencies\n",
    "model.add(LSTM(units=256, return_sequences=True))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(LSTM(units=128))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "# 4. Output layer: Predict the closing price\n",
    "model.add(Dense(units=1))\n",
    "\n",
    "# Compile the model with Adam optimizer and a lower learning rate\n",
    "optimizer = Adam(learning_rate=0.005)\n",
    "model.compile(optimizer=optimizer, loss='mse')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=64, validation_data=(X_test, y_test))\n",
    "\n",
    "# Predictions\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Evaluate performance\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "print(f\"Mean Squared Error: {mse}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
